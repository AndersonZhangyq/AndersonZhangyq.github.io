---
title: SVM
typora-copy-images-to: SVM
date: 2021-09-09 01:38:54
tags: 
categories:
    - ["Machine Learning"]
---
# 引入
SVM是一种常用的分类模型，其目标是寻找一个能将所有样本点准确划分的分离超平面（即分离超平面的两侧分别对应两个类别），优化目标是最大化几何间隔。为了能够应对线性不可分的数据集，引入了核技巧，其核心是将样本点映射到一个高维空间，认为低维空间不可分的数据集总能在高维空间中找到一个分离超平面将他们区分开来。

# 原理
假定我们有如下数据集，
$$
D=\lbrace (x_11, x_12, \cdots, x_1d, y_1), \cdots, (x_n1, x_n2, \cdots, x_nd, y_n) \rbrace
$$
数据集中每个样本$i$都有$d$个特征，$y_i$表示样本$i$所属的类别。简单起见，我们以二分类问题为例，此时有$y_i\in \lbrace0, 1\rbrace$。

## 函数间隔和几何间隔
我们用$w\cdot x+b$来表示一个分离超平面，并作如下规定：若$y_i=+1$，则$w\cdot x_i+b > 0$；若$y_i=-1$，则$w\cdot x_i+b < 0$。

显然，对于任意的$x_i$，都有$y_i(w\cdot x_i+b) > 0$。这里，$y_i(w\cdot x_i+b)$就是函数间隔。函数间隔描述了两件事情：

1. 函数间隔前的符号反映了样本是否被正确分类。如果大于0，那么这个样本就被正确分类了；
2. 函数间隔的绝对值描述了这个样本属于当前类的确信度，绝对值越大确信度越高。

因此，我们可以用函数间隔来评价分离超平面的优劣，更进一步，我们用数据集中的最小函数间隔来评价分离超平面的优劣。我们希望找到最小函数间隔尽可能大的分离超平面。

但是函数间隔有一个问题，当我们成倍扩大$w$和$b$时，函数间隔也会随之变大，这显然不是我们想要的，所以引入几何间隔的概念：
$\frac{y_i(w\cdot x_i+b)}{||w||_2}$。可见，相比于函数间隔，几何间隔用$w$的2范数做了归一化。（在几何意义上，$\frac{|w\cdot x_i+b|}{||w||_2}$表示样本点到分离超平面的距离，这大概就是几何间隔的由来）。

给定数据集D，我们定义分离超平面关于数据集D的几何间隔为所有样本点距离分离超平面的最小几何间隔，即
$$
\gamma = \min_{i=1,2,\cdots,N}{\frac{y_i(w\cdot x_i+b)}{||w||_2}}
$$

## 决策函数
如果我们找到了想要的超平面，即
$$
(w^*, b^*) = \underset{w, b}{\operatorname{argmin}}\gamma
$$
那么决策函数可以写成$sign(w^*\cdot x_i+b^*)$

## 支持向量和间隔
令$\beta > 0$，如果存在样本点$x_j$使得下式中的等号成立
$$
\left \lbrace 
\begin{align*}
w\cdot x_a+b &\ge \beta,\ y_a = 1 \\
w\cdot x_b+b &\le -\beta,\ y_b = -1
\end{align*}
\right .
$$
那么这些样本点对应的特征向量就被称之为支持向量。间隔定义为两个异类支持向量到分离超平面的距离，即
$$
\gamma = \frac{2\beta}{||w||_2}
$$

## 间隔最大化
间隔最大化意味着，找到的分离超平面能以最大的确信度将样本集分为两类。我们的优化目标可以表示为
$$
\begin{align*}
\max_{w,b}\ &\frac{2\beta}{||w||_2} \\
s.t.\ &y_i(w\cdot x_i+b) \ge \beta\ \ i = 1,2,\cdots,N
\end{align*}
$$
显然，$\beta$的取值和优化目标无关，上式可以重写为
$$
\begin{align*}
\min_{w,b}\ &{\frac{1}{2}||w||^2} \\
s.t.\ &y_i(w\cdot x_i+b) \ge 1\ \ i = 1,2,\cdots,N
\end{align*}
$$

## 对偶问题
应用拉格朗日乘子法，将约束写入到目标函数中，得到
$$
L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_i^N{\alpha_i(1 - y_i(w\cdot x_i+b))}
$$
原问题的解等价于
$$
\max_{\alpha}{\min_{w,b}\ {L(w,b,\alpha)}}
$$
首先求解极小化问题，得：
$$
\begin{align*}
\triangledown L_w = w - \sum_i^N{\alpha_i y_i} \\
\end{align*}
$$