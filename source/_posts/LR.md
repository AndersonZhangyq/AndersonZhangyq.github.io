---
title: LR
typora-copy-images-to: LR
date: 2021-09-07 14:12:55
tags:
categories:
    - ["Machine Learning"]
---
# 引入
逻辑斯特回归的输入是样本的特征，输出是样本属于正例的概率，可见他的输出是连续值。设定一个阈值之后，逻辑斯特回归就可以作为一个分类模型来使用。

# 原理
## 形式
假定我们有如下数据集，
$$
D=\lbrace (x_11, x_12, \cdots, x_1d, y_1), \cdots, (x_n1, x_n2, \cdots, x_nd, y_n) \rbrace
$$
数据集中每个样本$i$都有$d$个特征，$y_i$表示样本$i$所属的类别。简单起见，我们以二分类问题为例，此时有$y_i\in \lbrace0, 1\rbrace$。

### 连续值的离散化

考虑线性回归模型$$w^Tx+b$$，模型输出为连续值，值域为$R$，显然无法直接得到分类结果，因此我们需要对连续值做离散化处理。

离散化处理的一种方式是使用阶跃函数，但是阶跃函数的问题在于难以优化，导致很难找到一个合适的参数$w$。

如果我们可以找到一个自变量的定义域且值域为$(0,1)$为$R$的可微凸函数，那么我们就可以用这个函数来将线性回归模型的输出映射到$(0,1)$中，并认为映射后的值为某一个事件发生的概率。满足这个条件的其中一个函数为逻辑斯特函数，即
$$
y=\frac{1}{1+e^{-x}}
$$

### 几率

将逻辑斯特函数和线性回归模型组合在一起，得到
$$
y=\frac{1}{1+e^{-(w^Tx+b)}}
$$
令$y=P(Y=1|x)$，即$y$表示样本$x$属于类别1的概率，两边同取对数得
$$
\begin{align}
y&=\frac{1}{1+e^{-(w^Tx+b)}} \\
ln\frac{y}{1-y}&=w^Tx+b
\end{align}
$$
几率定义为事件发生的概率比上事件不发生的概率。可见，线性模型的输出表示的是对数几率。

## 损失函数

### 极大似然估计

想要求得参数$w$，那么可以用极大似然估计法得到对数似然函数。令$p(x_i)=P(Y=1|x_i)$，那么$1-p(x_i)=P(Y=0|x_i)$，则有
$$
\begin{align}
L(w)&=\prod_i^n{p(x_i)^{y_i}(1-p(x_i))^{1-y_i}}\\
ln(L(w))&=\sum_i^n{(y_ip(x_i)+(1-y_i)(1-p(x_i)))}
\end{align}
$$
可见，对数似然函数和我们熟悉的交叉熵损失互为相反数，所以最小化交叉熵损失和最大化对数似然函数是一致的，最小化交叉熵损失就是在做极大似然估计。至于优化方法，可以用梯度下降、牛顿法、拟牛顿法等常见优化方法。

# 其他

## 均方差函数

具体推导过程可参考[Why not Mean Squared Error(MSE) as a loss function for Logistic Regression?](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c)，总计来说就是使用交叉熵可以保证优化目标是个凸函数（因此局部最优解就是局部最优解），但是用均方差函数得到的优化目标是个非凸函数，往往会陷入到局部最优解中。